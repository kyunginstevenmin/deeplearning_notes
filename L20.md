

**GPT2 models aim to do zero-shot learning. Learning without labelled data.**

**Task is prompted by natural language rather than one-hot encoded tasks.**
natural language representation of tasks is an open set problem, rather than a closed set problem of one hot encoding representation of tasks.
- there are infinite amounts of tasks that the model can do.

Scaling in GPT2:
	Larger models/models with more parameters have less persplexity.
	The greater the capacity, the better the perplexity.
	
Generalization error of LLM's continues to decrease with increasing data.
![[Pasted image 20240716155010.png]]
Scaling study: kaplan 2020
- performance depends strongly on scale, weakly on model shape
- power law relationship of test loss with dataset size, model size, compute
- effects of some hyperparameters can be predicted prior training.
	- optimizer, model depth, lstm vs transformer

consequence of scaling law: larger the model, the better the performance.

Emergengt abilities: 
	From question: why to larger models perform better? what can they do that smaller models can't do?
	definition: abilities present in larger models that are not present in smaller models
	causes of emergent abilities are not fully known. Scale is one of the causes.

Main drivers of LLM era: scaling causing emergent abilities, whose causes are not yet fully known.

## Large language models
### LLM realization - architecture
1. encoder only: BERT
	1. pre-training: masked language modeling.
	2. generates length equal to the input length.'

2. decoder-only: GPT
	1. can generate sequence of any length because its autoregressive.

3. encoder-decoder
- masked span prediction:

Masked span prediction:
- mask continuous sequence set of tokens in input
- predict the masked span from decoder

![[Pasted image 20240716161808.png]]
### which architecture is the best?
empirical observation: causal decoder model performs the best.

### example of decoder only model: llama 2

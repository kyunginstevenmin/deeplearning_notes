## Sequence to sequence models:
### case 1. order synchronous models
- nth output corresponds to nth segment of input
For order synchronous, time asynchronous sequence models, we align the output sequence with the input sequence then train the model. 
### case 2. No correspondence between input andoutput.
However, this is hard in other sequence to sequence models. When there is no order synchrony, or if you have to analyze entire input and produce the output.
For example:
- question and answers have no 1 to 1 corresondence.
- translations often don't have order correspondence.

## Language models
Language models model the probability distribution of token sequences in a language. Tokens are any unit of word that makes up a sequence. For example for a sentence, words are the tokens, and sequence of token makes up sentences.

The probability of an entire word sequence is constructed via Bayes rule. We get the product of the conditional probability of the next word, given all previous sequence of words.
![image](<Pasted image 20240709110707.png>)
**So language models construct new words by answering the question: given the sequence of words, whats the next probably word?**
For example, given the sentence and letters, what's the next probable word or letter?
![image](<Pasted image 20240709110856.png>)
### Representing words
Words are represented using one-hot vectors.
Using one-hot vectors has a problem of large dimensionality. It has another dimension for every word.
![image](<Pasted image 20240709111325.png>)

But despite the large dimensionality problem, it has the advantage:
- that it makes no prior assumptions about the relationship between words. 
	-  This is because the length of all words vectors are equal, 
- It makes no assumption about the importance of each word.
	- The distance between every pair of word vectors are equal.

To deal with the dimensionality problem, we project the word vectors into a lower dimensional subspace. The lower dimensional subspace is known as [[Word embeddings>).
	If the linear transformations are properly learned, the distances between projected points will capture semantic relations between the words.
	In neural network terms, this linear transformation is a fully connected linear layer with shared parameters. This is a 1D convolution, or a TDNN.
	This is the process of learning the right representation of the words.

![image](<Pasted image 20240709111705.png>)

Example of learning the right representation of words vai projection:
- the vector between the projected vectors of the words capture relationship between country and their capital. 
![image](<Pasted image 20240709112019.png>)
### Generating language:
Language is generated from a trained model in the following manner:
- provide few initial words as input.
- at the last input, the network produces probability distribution of possible words.
- A word is drawn from the probability distribution, as also becomes next word in the series.
![image](<Pasted image 20240709114812.png>)

## We can model order asynchronous models as delayed sequence to sequence models.
![image](<Pasted image 20240709120526.png>)
There are two parts to this model:
1. The encoder:
	1. The encoder analyzes the entire input sequence, ad generates a hidden representation of the entire input sequence.
	2. The final hidden state captures all information about the input sequence. This is passed on to the decoder, which is used to generate an output.
2. The decoder:
	1. Uses the current hidden state to generate an output.

Problem with this method:
1. All information about the input is in single final hidden state.
	1. Attention models deal with this problem.
2. Output at time t is not influence by the output at time t-1.
	1. **Self-referential delayed sequence to sequence** models deal with this problem.

![image](<Pasted image 20240709115220.png>)

## Simple translation model/self-referential
This differs from the delayed sequence to sequence model, in that the output at t-1 feeds into processing the hidden state at t, and thus the output at t.
- so this model uses hidden state provided by decoder and the predicted output at t-1 to predict output at t.
- But when you train the model, we use the desired output at t-1 to predict output at t.
![image](<Pasted image 20240709120913.png>)
	**Final hidden representation:**
		In red is the final hidden representation stored information about input sequence.
	The first step of generator is provided start of sequence code as input.
	Output:
		We generate probability distribution of word conditional on hidden state provided by encoder and all previous input sequence.
		![image](<Pasted image 20240709121057.png>)
	We draw the most probably word according to the distribution.
	This word becomes the subsequent input for t+1, during testing.
		During training, we don't use the output predicted by the model, but use the desired output.
	We continue until <eos> is drawn
	
### How Do you draw words so that we produce most likely output sequence?
The probability of the final output sequence is modelled by the product of the conditional probabilities of output at time t given output sequent until time t-1 and all input sequence.  This product gives the joint probability of the entire sequence. 
![image](<Pasted image 20240709121423.png>)

![image](<Pasted image 20240709121542.png>)


There are two ways of drawing at each time t so that we end with sequence with greatest probability. Greedy drawing and drawing with pruning.

Greedy drawing just draws on the word with the highest probability. 
	Problem: word chosen effects the probability of words further down, and we don't know a priori how the word drawn now will affect the future.
Solution: Random Sampling
	 Instead of drawing word with highest probability, use the entire distribution to draw the word. 
	 But this still doesn't ensure final sequence with greatest probability.

Solution 2. Maintain all possible outputs at each time step and fork the network.
Problem: this blows up the computation.
![image](<Pasted image 20240709122049.png>)

Solution: Prune
![image](<Pasted image 20240709122355.png>)
-  you keep only top K scoring forks at each time step.
	- THe score is based on the entire sequence unto that time t. So you update the probability at time t by finding the joint probability of the sequence up to time t. And prune all pathways and retain only top k scoring forks.
- Conditions for termination:
	- Terminate when current most likely path overall ends in <eos>
	- You can also generate multiple paths and get N-best outputs.


## Training the system
Given training instance (X,D): where X is the input sequence and D is the desired output sequence.
for each training instance:
- **Forward path**: compute output of the network for (X,D)
	- X is used to generate a hidden representation in the encoder.
	- elements in D are used in sequential manner. To generate output at t, we provide the desired output at t-1 as input to the generator.
- Backward path: Compute divergence between selected words of the desired target D and the actual output Y.
	- Question: Is the divergence between the probability vector output Y and the desired target D? The loss would be the -log probability of the Y for the target token D.
- update parameters 

Tips and tricks: reverse input.
![image](<Pasted image 20240709122920.png>)


Examples: generating image captions

- image to sequence problem.
- Decoding Image using CNN:
	- CNN is used to generate embedding for image. Embedding is a reduced dimension representation of the input.
	- 
- This embedding is inputted to a decoder of a sequence to sequence model (LSTM).
![image](<Pasted image 20240709124132.png>)